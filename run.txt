There are two ways of running the code.

1. run locally directly

    1. Compile the scala object and using sbt and create a jar:
	            sbt package
	  2. Running the code:
	            sbt 'runMain org.vam.spark.Logistic_Apache_Spark glass.txt'
	        
2. run it on the hdfs yarn cluster
	1. <spark installed dir>/bin/spark-submit --class org.vam.spark.Logistic_Apache_Spark --master yarn --deploy-mode 		cluster --executor-memory 7g --queue default --num-executors 10 --driver-memory 7g ./'logistic.jar' glass.txt
	2. Output can be see by looking at the file.
	hadoop fs -ls /user/vardhaman/Logistic_output/
